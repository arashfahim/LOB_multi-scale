{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57b7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import cvxpy as cp\n",
    "from scipy.optimize import fsolve\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10664411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1000) tensor(0.0100)\n"
     ]
    }
   ],
   "source": [
    "M=1000\n",
    "ite=10\n",
    "dim_n=5\n",
    "T=1\n",
    "T_C=1\n",
    "T_F=0.1\n",
    "delta_c = torch.tensor(T_C/ite)\n",
    "delta_f=torch.tensor(T_F/ite)\n",
    "print(delta_c,delta_f)\n",
    "#delta=torch.tensor(0.01)\n",
    "sigma=0.1\n",
    "sigma1=1e-3\n",
    "sigma2=1e-2\n",
    "theta1=1\n",
    "theta2=1\n",
    "mu1=1e-2#kappa\n",
    "mu2=5 #rho\n",
    "A0=0.0\n",
    "gamma=0.0\n",
    "X0=10000\n",
    "neuron_model_psi=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc8985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f= torch.nn.Sequential(\n",
    "    torch.nn.Linear(dim_n, neuron_model_psi),\n",
    "    torch.nn.ReLU(),\n",
    "    #torch.nn.Linear(neuron_model_psi, neuron_model_psi),\n",
    "    #torch.nn.ReLU(),\n",
    "    # torch.nn.Linear(neuron_model_psi, neuron_model_psi_2),\n",
    "    # torch.nn.ReLU(),\n",
    "    torch.nn.Linear(neuron_model_psi, neuron_model_psi),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(neuron_model_psi,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e9d4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, 'model_N10.pth')\n",
    "model_coarse = torch.load('LOB_stoc_N10.pth')\n",
    "#model_coarse.eval()(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6913910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa_rho_sim(M,ite,theta1,mu1,sigma1,theta2,mu2,sigma2,delta):\n",
    "    kappa=torch.zeros([M,ite+1])\n",
    "    kappa[:,0]=mu1\n",
    "    rho=torch.zeros([M,ite+1])\n",
    "    rho[:,0]=mu2\n",
    "\n",
    "    for i in range (1,ite+1):\n",
    "        kappa[:,i]=kappa[:,i-1]+theta1*(mu1-kappa[:,i-1])*delta+np.sqrt(delta)*sigma1*np.random.rand(M)  # Xn+1=Xn+θ(μ−Xn)Δt+σΔWn\n",
    "        rho[:,i]=rho[:,i-1]+theta2*(mu2-rho[:,i-1])*delta+np.sqrt(delta)*sigma2*np.random.rand(M)\n",
    "    return kappa,rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca11e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(x3,psi,i,delta,kappa,rho):\n",
    "  t=x3[:,0]+delta\n",
    "  kap=kappa[:,i+1]\n",
    "  rh=rho[:,i+1]\n",
    "#   kappa=x3[:,3]+theta1*(mu1-x3[:,3])*delta+torch.sqrt(delta)*sigma1*W1[i]\n",
    "#   rho=x3[:,4]+theta2*(mu2-x3[:,4])*delta+torch.sqrt(delta)*sigma1*W2[i]\n",
    "  D = (x3[:,1]+kap*psi)*torch.exp(-rh*delta)\n",
    "  #print('d',D.shape)\n",
    "  R= x3[:,2]-psi\n",
    "  #print(kappa.shape,rho.shape)\n",
    "  up=torch.cat((t.unsqueeze(1),D.unsqueeze(1),R.unsqueeze(1),kap.unsqueeze(1),rh.unsqueeze(1)),dim=1)\n",
    "  return up\n",
    "\n",
    "def loss_func(x2,psi):\n",
    "  loss=(x2[:,1]*psi+(x2[:,3]/2.0)*torch.pow(psi,2))\n",
    "  #print('loss',loss.shape)\n",
    "  return loss\n",
    "\n",
    "def unit(x1,model,i,delta,kappa,rho):\n",
    "    psi=model(x1).squeeze(1)\n",
    "    los=loss_func(x1,psi)\n",
    "    upd=update(x1,psi,i,delta,kappa,rho)\n",
    "    #print(upd)\n",
    "    return psi,los,upd\n",
    "\n",
    "def loss_func_total(u,model,delta,kappa,rho):\n",
    "  loss=torch.zeros(M,ite)\n",
    "  psi=torch.zeros(M,ite)\n",
    "  for i in range(ite+1):\n",
    "    if(i!=ite):\n",
    "      psi_run,loss_run,u_run=unit(u,model,i,delta,kappa,rho)\n",
    "      #print('los func=',psi_run.shape,loss_run.shape)\n",
    "      loss[:,i]=loss_run\n",
    "      #print(loss)\n",
    "      psi[:,i]=psi_run\n",
    "      #print(psi)\n",
    "      u=u_run\n",
    "      #print(u)\n",
    "    else:\n",
    "      #print(torch.sum(psi,dim=1),R.squeeze(1))\n",
    "      psi_ter=R.squeeze(1)-torch.sum(psi,dim=1)\n",
    "      loss_ter=loss_func(u,psi_ter)\n",
    "      #print('ter',loss_ter.shape)\n",
    "  #print(torch.sum(loss,dim=1))\n",
    "  loss=torch.sum(loss,dim=1)+loss_ter\n",
    "  #print(loss.shape)\n",
    "  return torch.mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6c73a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kappa,rho=kappa_rho_sim(M,ite,theta1,mu1,sigma1,theta2,mu2,sigma2,delta_f) \n",
    "# print(kappa,rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc239cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_coarse(X0,M1): \n",
    "    t_test=torch.zeros([M1,1])\n",
    "    D_test=torch.zeros([M1,1])\n",
    "    #D_test=torch.FloatTensor(M,1).uniform_(0.1,0.5)   #Price impact D_t\n",
    "    #R_test=torch.FloatTensor(M1,1).uniform_(50,100)   #remaining balance R_t   #To get a positive solution R_t has to be greater than D_t\n",
    "    R_test=torch.linspace(X0*0.95, X0*1.05, steps=M1).unsqueeze(1)\n",
    "    kappa,rho=kappa_rho_sim(M1,ite,theta1,mu1,sigma1,theta2,mu2,sigma2,delta_c)\n",
    "    #(M,ite,theta1,mu1,sigma1,theta2,mu2,sigma2)\n",
    "    x_test=torch.cat((t_test,D_test,R_test,kappa[:,0].unsqueeze(1),rho[:,0].unsqueeze(1)),dim=1)\n",
    "    print('Input=',x_test,'\\n')\n",
    "    a=torch.zeros(M1,ite+1)\n",
    "    #print(a.shape)\n",
    "    for i in range(ite+1):\n",
    "      if(i!=ite):\n",
    "        #print(x_test)\n",
    "        soln_pred=model_coarse.eval()(x_test).squeeze(1).detach()\n",
    "        a[:,i]=soln_pred\n",
    "        x_test=update(x_test,soln_pred,i,delta_c,kappa,rho)\n",
    "        #print(x_test,'\\n')\n",
    "      else:\n",
    "        a[:,i]=R_test.squeeze(1)-torch.sum(a,dim=1)\n",
    "        \n",
    "    return x_test,a\n",
    "\n",
    "# x1=R_test.squeeze().numpy()\n",
    "# c_pred=np.zeros(M1)\n",
    "\n",
    "# for i in range(M1):\n",
    "#   print('size of total order=',x1[i])\n",
    "\n",
    "\n",
    "#   print('predicted soln=',a[i,:],'sum of all execution=',torch.sum(a[i,:]).detach())\n",
    "#   #pred_cost=cost(a[i,:].detach().numpy())\n",
    "# #   print('predicted cost=',pred_cost,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4886db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input= tensor([[0.0000e+00, 0.0000e+00, 9.5000e+03, 1.0000e-02, 5.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 9.7500e+03, 1.0000e-02, 5.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+04, 1.0000e-02, 5.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0250e+04, 1.0000e-02, 5.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0500e+04, 1.0000e-02, 5.0000e+00]]) \n",
      "\n",
      "coarse soution= tensor([[1841.6835,  752.4590,  698.4988,  695.9855,  685.2945,  673.0715,\n",
      "          668.5593,  657.7728,  654.0044,  646.6080, 1526.0620],\n",
      "        [1871.3734,  764.5327,  709.9633,  693.7514,  693.2693,  690.0809,\n",
      "          684.4357,  671.9454,  665.9000,  661.3616, 1643.3867],\n",
      "        [1901.0638,  789.0984,  731.9882,  722.5742,  709.9712,  704.5897,\n",
      "          698.1665,  688.4902,  683.6656,  669.8616, 1700.5312],\n",
      "        [1930.7534,  800.0925,  741.8921,  718.9171,  723.3641,  709.2153,\n",
      "          704.4932,  691.5734,  678.7670,  676.9131, 1874.0186],\n",
      "        [1960.4436,  808.0334,  749.4906,  712.5776,  718.6760,  712.5457,\n",
      "          703.3227,  689.9700,  686.6643,  675.6217, 2082.6543]])\n",
      "tensor([[0.0000e+00, 7.5755e+00, 1.1938e+03, 1.0000e-02, 5.0000e+00],\n",
      "        [0.0000e+00, 8.3338e-01, 1.4710e+03, 1.0000e-02, 5.0000e+00],\n",
      "        [0.0000e+00, 6.9205e-01, 1.9788e+03, 1.0000e-02, 5.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 7.8594e+00, 1.4255e+03, 1.0000e-02, 5.0000e+00],\n",
      "        [0.0000e+00, 8.1399e+00, 8.2844e+02, 1.0000e-02, 5.0000e+00],\n",
      "        [0.0000e+00, 1.5222e+00, 1.3165e+03, 1.0000e-02, 5.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "x_c,c_soln=testing_coarse(X0,5)\n",
    "# print(x_c,c_soln,torch.sum(a,1))\n",
    "D_max=torch.max(x_c[:,1])\n",
    "R_min=torch.min(c_soln)\n",
    "R_max=torch.max(c_soln)\n",
    "t=torch.zeros([M,1])\n",
    "kappa,rho=kappa_rho_sim(M,ite,theta1,mu1,sigma1,theta2,mu2,sigma2,delta_f) \n",
    "D=torch.FloatTensor(M,1).uniform_(0.0,D_max)\n",
    "#D=torch.FloatTensor(M,1).uniform_(0.1,0.5)   #Price impact D_t\n",
    "R=torch.FloatTensor(M,1).uniform_(R_min,R_max)   #remaining balance R_t   #To get a positive solution R_t has to be greater than D_t\n",
    "x=torch.cat((t,D,R,kappa[:,0].unsqueeze(1),rho[:,0].unsqueeze(1)),dim=1)\n",
    "print('coarse soution=',c_soln)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1db500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch 0 the mean cost is 17374.986328125.\n",
      "At epoch 100 the mean cost is 14642.25390625.\n",
      "At epoch 200 the mean cost is 14636.439453125.\n",
      "At epoch 300 the mean cost is 14622.267578125.\n",
      "At epoch 400 the mean cost is 14580.712890625.\n",
      "At epoch 500 the mean cost is 14520.5341796875.\n",
      "At epoch 600 the mean cost is 14493.458984375.\n",
      "At epoch 700 the mean cost is 14483.7001953125.\n",
      "At epoch 800 the mean cost is 14478.9404296875.\n",
      "At epoch 900 the mean cost is 14476.240234375.\n",
      "At epoch 1000 the mean cost is 14474.6669921875.\n",
      "At epoch 1100 the mean cost is 14473.7626953125.\n",
      "At epoch 1200 the mean cost is 14473.2431640625.\n",
      "At epoch 1300 the mean cost is 14472.935546875.\n",
      "At epoch 1400 the mean cost is 14472.73828125.\n",
      "At epoch 1500 the mean cost is 14472.5859375.\n",
      "At epoch 1600 the mean cost is 14472.4609375.\n",
      "At epoch 1700 the mean cost is 14472.345703125.\n",
      "At epoch 1800 the mean cost is 14472.2353515625.\n",
      "At epoch 1900 the mean cost is 14472.125.\n",
      "At epoch 2000 the mean cost is 14472.017578125.\n",
      "At epoch 2100 the mean cost is 14471.9130859375.\n",
      "At epoch 2200 the mean cost is 14471.8095703125.\n",
      "At epoch 2300 the mean cost is 14471.70703125.\n",
      "At epoch 2400 the mean cost is 14471.60546875.\n",
      "At epoch 2500 the mean cost is 14471.5.\n",
      "At epoch 2600 the mean cost is 14471.3935546875.\n",
      "At epoch 2700 the mean cost is 14471.2890625.\n",
      "At epoch 2800 the mean cost is 14471.2158203125.\n",
      "At epoch 2900 the mean cost is 14471.0732421875.\n",
      "At epoch 3000 the mean cost is 14471.1494140625.\n",
      "At epoch 3100 the mean cost is 14470.8466796875.\n",
      "At epoch 3200 the mean cost is 14471.935546875.\n",
      "At epoch 3300 the mean cost is 14470.6064453125.\n",
      "At epoch 3400 the mean cost is 14470.66796875.\n",
      "At epoch 3500 the mean cost is 14470.3486328125.\n",
      "At epoch 3600 the mean cost is 14470.22265625.\n",
      "At epoch 3700 the mean cost is 14470.1171875.\n",
      "At epoch 3800 the mean cost is 14469.9296875.\n",
      "At epoch 3900 the mean cost is 14469.826171875.\n",
      "At epoch 4000 the mean cost is 14469.6240234375.\n",
      "At epoch 4100 the mean cost is 14469.58203125.\n",
      "At epoch 4200 the mean cost is 14469.294921875.\n",
      "At epoch 4300 the mean cost is 14469.236328125.\n",
      "At epoch 4400 the mean cost is 14468.9443359375.\n",
      "At epoch 4500 the mean cost is 14468.8203125.\n",
      "At epoch 4600 the mean cost is 14468.5703125.\n",
      "At epoch 4700 the mean cost is 14468.38671875.\n",
      "At epoch 4800 the mean cost is 14468.1689453125.\n",
      "At epoch 4900 the mean cost is 14467.9716796875.\n",
      "At epoch 5000 the mean cost is 14468.1630859375.\n",
      "At epoch 5100 the mean cost is 14467.5419921875.\n",
      "At epoch 5200 the mean cost is 14467.341796875.\n",
      "At epoch 5300 the mean cost is 14467.126953125.\n",
      "At epoch 5400 the mean cost is 14467.42578125.\n",
      "At epoch 5500 the mean cost is 14466.6796875.\n",
      "At epoch 5600 the mean cost is 14466.4755859375.\n",
      "At epoch 5700 the mean cost is 14466.2998046875.\n",
      "At epoch 5800 the mean cost is 14466.71875.\n",
      "At epoch 5900 the mean cost is 14474.3095703125.\n",
      "At epoch 6000 the mean cost is 14465.6640625.\n",
      "At epoch 6100 the mean cost is 14465.4990234375.\n",
      "At epoch 6200 the mean cost is 14465.3564453125.\n",
      "At epoch 6300 the mean cost is 14465.1650390625.\n",
      "At epoch 6400 the mean cost is 14465.0029296875.\n",
      "At epoch 6500 the mean cost is 14464.8681640625.\n",
      "At epoch 6600 the mean cost is 14464.748046875.\n",
      "At epoch 6700 the mean cost is 14464.6357421875.\n",
      "At epoch 6800 the mean cost is 14464.5673828125.\n",
      "At epoch 6900 the mean cost is 14464.8837890625.\n",
      "At epoch 7000 the mean cost is 14464.83984375.\n",
      "At epoch 7100 the mean cost is 14466.9140625.\n",
      "At epoch 7200 the mean cost is 14464.6416015625.\n",
      "At epoch 7300 the mean cost is 14464.19921875.\n",
      "At epoch 7400 the mean cost is 14464.1513671875.\n",
      "At epoch 7500 the mean cost is 14464.1259765625.\n",
      "At epoch 7600 the mean cost is 14464.2880859375.\n",
      "At epoch 7700 the mean cost is 14464.916015625.\n",
      "At epoch 7800 the mean cost is 14464.078125.\n",
      "At epoch 7900 the mean cost is 14464.0478515625.\n",
      "At epoch 8000 the mean cost is 14464.00390625.\n",
      "At epoch 8100 the mean cost is 14464.0439453125.\n",
      "At epoch 8200 the mean cost is 14463.984375.\n",
      "At epoch 8300 the mean cost is 14463.9814453125.\n",
      "At epoch 8400 the mean cost is 14463.970703125.\n",
      "At epoch 8500 the mean cost is 14463.9638671875.\n",
      "At epoch 8600 the mean cost is 14463.958984375.\n",
      "At epoch 8700 the mean cost is 14463.9521484375.\n",
      "At epoch 8800 the mean cost is 14463.94140625.\n",
      "At epoch 8900 the mean cost is 14463.9404296875.\n",
      "At epoch 9000 the mean cost is 14463.9423828125.\n",
      "At epoch 9100 the mean cost is 14463.9482421875.\n",
      "At epoch 9200 the mean cost is 14463.9326171875.\n",
      "At epoch 9300 the mean cost is 14463.96484375.\n",
      "At epoch 9400 the mean cost is 14464.166015625.\n",
      "At epoch 9500 the mean cost is 14464.181640625.\n",
      "At epoch 9600 the mean cost is 14465.6181640625.\n",
      "At epoch 9700 the mean cost is 14464.1083984375.\n",
      "At epoch 9800 the mean cost is 14464.095703125.\n",
      "At epoch 9900 the mean cost is 14465.9990234375.\n",
      "At epoch 10000 the mean cost is 14464.60546875.\n",
      "At epoch 10100 the mean cost is 14464.1064453125.\n",
      "At epoch 10200 the mean cost is 14464.1435546875.\n",
      "At epoch 10300 the mean cost is 14464.1748046875.\n",
      "At epoch 10400 the mean cost is 14464.01953125.\n",
      "At epoch 10500 the mean cost is 14463.919921875.\n",
      "At epoch 10600 the mean cost is 14464.1220703125.\n",
      "At epoch 10700 the mean cost is 14463.9189453125.\n",
      "At epoch 10800 the mean cost is 14463.916015625.\n",
      "At epoch 10900 the mean cost is 14463.9267578125.\n",
      "At epoch 11000 the mean cost is 14463.9677734375.\n",
      "At epoch 11100 the mean cost is 14464.169921875.\n",
      "At epoch 11200 the mean cost is 14463.919921875.\n",
      "At epoch 11300 the mean cost is 14463.919921875.\n",
      "At epoch 11400 the mean cost is 14463.9365234375.\n",
      "At epoch 11500 the mean cost is 14464.2646484375.\n",
      "At epoch 11600 the mean cost is 14464.8388671875.\n",
      "At epoch 11700 the mean cost is 14467.1767578125.\n",
      "At epoch 11800 the mean cost is 14464.4736328125.\n",
      "At epoch 11900 the mean cost is 14464.4453125.\n",
      "At epoch 12000 the mean cost is 14464.0146484375.\n",
      "At epoch 12100 the mean cost is 14463.9599609375.\n",
      "At epoch 12200 the mean cost is 14463.94921875.\n",
      "At epoch 12300 the mean cost is 14463.927734375.\n",
      "At epoch 12400 the mean cost is 14463.9716796875.\n",
      "At epoch 12500 the mean cost is 14466.41796875.\n",
      "At epoch 12600 the mean cost is 14464.3349609375.\n",
      "At epoch 12700 the mean cost is 14464.2509765625.\n",
      "At epoch 12800 the mean cost is 14464.0654296875.\n",
      "At epoch 12900 the mean cost is 14467.255859375.\n",
      "At epoch 13000 the mean cost is 14466.005859375.\n",
      "At epoch 13100 the mean cost is 14464.7314453125.\n",
      "At epoch 13200 the mean cost is 14463.9189453125.\n",
      "At epoch 13300 the mean cost is 14464.15625.\n",
      "At epoch 13400 the mean cost is 14464.0244140625.\n",
      "At epoch 13500 the mean cost is 14464.251953125.\n",
      "At epoch 13600 the mean cost is 14465.255859375.\n",
      "At epoch 13700 the mean cost is 14464.2138671875.\n",
      "At epoch 13800 the mean cost is 14463.9169921875.\n",
      "At epoch 13900 the mean cost is 14463.9228515625.\n",
      "At epoch 14000 the mean cost is 14463.9306640625.\n",
      "At epoch 14100 the mean cost is 14463.984375.\n",
      "At epoch 14200 the mean cost is 14464.3974609375.\n",
      "At epoch 14300 the mean cost is 14463.9296875.\n",
      "At epoch 14400 the mean cost is 14463.916015625.\n",
      "At epoch 14500 the mean cost is 14463.923828125.\n",
      "At epoch 14600 the mean cost is 14463.98046875.\n",
      "At epoch 14700 the mean cost is 14467.0322265625.\n",
      "At epoch 14800 the mean cost is 14465.7470703125.\n",
      "At epoch 14900 the mean cost is 14463.9921875.\n",
      "time elapsed= 123.49524641036987\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "lr1 = 8e-3\n",
    "max_epoch = 15000\n",
    "optimizer = optim.Adam(model_f.parameters(), lr1)\n",
    "\n",
    "#print(psi.shape)\n",
    "for epoch in range(max_epoch):\n",
    "  optimizer.zero_grad()\n",
    "  cost=loss_func_total(x,model_f,delta_f,kappa,rho)\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "  #print(loss.item())\n",
    "  if (epoch % 100==0):\n",
    "    print(\"At epoch {} the mean cost is {}.\".format(epoch,cost.detach()))\n",
    "end=time.time()\n",
    "print('time elapsed=',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fa41ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fin=model_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33decdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coarse soln [1901.0638   789.0984   731.98816  722.5742   709.9712   704.5897\n",
      "  698.1665   688.49023  683.6656   669.8616  1700.5312 ] Total= 10000.001\n"
     ]
    }
   ],
   "source": [
    "coarse_soln=c_soln[2].numpy()\n",
    "print('coarse soln',coarse_soln, 'Total=',np.sum(coarse_soln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32ca41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pred(x3,psi,i,delta,kappa,rho):\n",
    "  t=x3[:,0]+delta\n",
    "  kap=kappa[i+1].reshape(1)\n",
    "  rh=rho[i+1].reshape(1)\n",
    "  D = (x3[:,1]+kap*psi)*torch.exp(-rh*delta)\n",
    "  R= x3[:,2]-psi\n",
    "  #print(t.shape,D.shape,R.shape,kap.shape,rh.shape)\n",
    "  up=torch.cat((t.unsqueeze(1),D.unsqueeze(1),R.unsqueeze(1),kap.unsqueeze(1),rh.unsqueeze(1)),dim=1)\n",
    "  return up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "809e4e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_fine_first_last(c_soln,ite):    \n",
    "    f_soln=[]\n",
    "    c=c_soln[2]\n",
    "    x_test=torch.tensor([[0.0,0.0,c[0],0.0010,5.000]])\n",
    "    x_test[0,2]=x_test[0,2]+c[ite]\n",
    "    for i in range(ite):       \n",
    "        if (i!=0):\n",
    "            x_test[0,2]=x_test[0,2]+c[i] #Adding previous remaining balance to the current balance         \n",
    "        for j in range(ite):\n",
    "            #print(i,x_test)\n",
    "            soln_pred=model_fin.eval()(x_test).squeeze(1).detach()\n",
    "            f_soln.append(soln_pred)\n",
    "            #print(soln_pred.shape)\n",
    "            x_test=update_pred(x_test,soln_pred,j,delta_f,kappa[1,],rho[1,])\n",
    "            #print(x_test[0,2])\n",
    "    f_soln.append(x_test[0,2].view(1))\n",
    "    f_soln= np.array([tensor.numpy() for tensor in f_soln]).reshape(len(f_soln))\n",
    "    return x_test,f_soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58c55082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine solution= [1680.6077      9.505421   59.39638    56.894398   56.224995   55.537647\n",
      "   54.915333   54.247334   53.81383    53.250935  436.59525    51.838154\n",
      "   62.87572    61.542316   60.699604   59.89629    59.16419    58.383533\n",
      "   57.85741    57.19315   412.67172    55.84026    65.9337     64.543396\n",
      "   63.619114   62.73897    61.935505   61.08054    60.494663   59.76386\n",
      "  410.606      58.383183   68.27054    66.820526   65.83473    64.896255\n",
      "   64.03871    63.127598   62.496174   61.715305  406.38342    60.325733\n",
      "   69.9772     68.48569    67.45537    66.47392    65.57672    64.62489\n",
      "   63.959896   69.65373   398.44745    61.94385    71.26575    69.74799\n",
      "   78.50223    90.77168    98.78439   103.558044   97.59466    78.90058\n",
      "  257.64807   123.571594  122.71504   110.76935    87.13356    83.80356\n",
      "   96.5981     94.46871    54.846256   30.607925  270.5868    158.65625\n",
      "  129.24263   108.39784    38.928677   28.296202   23.91917    21.935358\n",
      "   20.885292   20.197836  450.47382   151.37895    44.337852   30.25817\n",
      "   24.552345   22.062937   20.816341   20.05415    19.496979   19.02547\n",
      "  543.20166    60.3017     34.052773   25.728863   22.246338   20.629341\n",
      "   19.73846    19.134518   18.659      18.239264 -675.27966 ] Total= 10000.002\n"
     ]
    }
   ],
   "source": [
    "data,fine_soln=testing_fine_first_last(c_soln,ite)\n",
    "D_N=data[0,2].detach().numpy()\n",
    "\n",
    "print('\\nFine solution=', fine_soln,'Total=',np.sum(fine_soln))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed606c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_step1=np.linspace(0,1,num=11)\n",
    "t_step2=np.linspace(0,1,num=101)\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(t_step1,coarse_soln,color='red',width=0.01,label='coarse')\n",
    "plt.bar(t_step2,fine_soln,color='green',width=0.005,label='fine')\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('Optimal orders')\n",
    "plt.title('Forward Pass')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
