{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86743b5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.cbook' has no attribute '_get_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m misc\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtikzplotlib\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/matplotlib/__init__.py:964\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[1;32m    960\u001b[0m \u001b[38;5;66;03m# When constructing the global instances, we need to perform certain updates\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;66;03m# by explicitly calling the superclass (dict.update, dict.items) to avoid\u001b[39;00m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;66;03m# triggering resolution of _auto_backend_sentinel.\u001b[39;00m\n\u001b[1;32m    963\u001b[0m rcParamsDefault \u001b[38;5;241m=\u001b[39m _rc_params_in_file(\n\u001b[0;32m--> 964\u001b[0m     cbook\u001b[38;5;241m.\u001b[39m_get_data_path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatplotlibrc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;66;03m# Strip leading comment.\u001b[39;00m\n\u001b[1;32m    966\u001b[0m     transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m line: line[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m line,\n\u001b[1;32m    967\u001b[0m     fail_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(rcParamsDefault, rcsetup\u001b[38;5;241m.\u001b[39m_hardcoded_defaults)\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# Normally, the default matplotlibrc file contains *no* entry for backend (the\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# corresponding line starts with ##, not #; we fill on _auto_backend_sentinel\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# in that case.  However, packagers can set a different default backend\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# (resulting in a normal `#backend: foo` line) in which case we should *not*\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# fill in _auto_backend_sentinel.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.cbook' has no attribute '_get_data_path'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as np\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "import tikzplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "# import cvxpy as cp\n",
    "from scipy.optimize import fsolve\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffe4d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "M=2000\n",
    "ite=10\n",
    "dim_n=3\n",
    "T=1\n",
    "T_C=1\n",
    "T_F=0.1\n",
    "delta_c = torch.tensor(T_C/ite)\n",
    "delta_f=torch.tensor(T_F/ite)\n",
    "print(delta_c,delta_f)\n",
    "sigma=0.2\n",
    "mu=0.3\n",
    "A0=0.0\n",
    "gamma=0.0\n",
    "kappa=1.0\n",
    "rho=5\n",
    "X0=10000\n",
    "neuron_model_psi=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d81260",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f= torch.nn.Sequential(\n",
    "    torch.nn.Linear(dim_n, neuron_model_psi),\n",
    "    torch.nn.ReLU(),\n",
    "    #torch.nn.Linear(neuron_model_psi, neuron_model_psi),\n",
    "    #torch.nn.ReLU(),\n",
    "    # torch.nn.Linear(neuron_model_psi, neuron_model_psi_2),\n",
    "    # torch.nn.ReLU(),\n",
    "    torch.nn.Linear(neuron_model_psi, neuron_model_psi),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(neuron_model_psi,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, 'model_N10.pth')\n",
    "model_coarse = torch.load('model_N10_best.pth')\n",
    "#model_coarse.eval()(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f513dd4",
   "metadata": {},
   "source": [
    "## Value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6290cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = 500\n",
    "t_test=torch.zeros([M1,1])\n",
    "D_test=torch.zeros([M1,1])\n",
    "R_test=torch.linspace(X0*0.90, X0*1.1, steps=M1).unsqueeze(1)\n",
    "x_test=torch.cat((t_test,D_test,R_test),dim=1)\n",
    "a=torch.zeros(M1,ite+1)\n",
    "for i in range(ite+1):\n",
    "    if(i!=ite):\n",
    "        #print(x_test)\n",
    "        soln_pred=model_coarse.eval()(x_test).squeeze(1).detach()\n",
    "        a[:,i]=soln_pred\n",
    "        x_test=update(x_test,soln_pred,delta_c)\n",
    "        #print(x_test,'\\n')\n",
    "    else:\n",
    "        a[:,i]=R_test.squeeze(1)-torch.sum(a,dim=1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(a.clone().detach().numpy(), columns=[\"t=\"+str(i) for i in range(ite+1)])\n",
    "# sns.displot(data=df,x=\"t=0\")\n",
    "penguins = sns.load_dataset(\"penguins\")\n",
    "sns.displot(data=penguins, x=\"flipper_length_mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e87fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(x3,psi,delta):\n",
    "  t=x3[:,0]+delta\n",
    "  #print(psi.squeeze(1).shape,x1[:,1].shape)\n",
    "  D = (x3[:,1]+kappa*psi)*torch.exp(-rho*delta)\n",
    "  R= x3[:,2]-psi\n",
    "  #print(R.shape)\n",
    "  up=torch.cat((t.unsqueeze(1),D.unsqueeze(1),R.unsqueeze(1)),dim=1)\n",
    "  #print('up shape=', up.shape)\n",
    "  return up\n",
    "\n",
    "def loss_func(x2,psi):\n",
    "  loss=(x2[:,1]*psi+(kappa/2.0)*torch.pow(psi,2))\n",
    "  return loss\n",
    "\n",
    "def unit(x1,model,delta):\n",
    "    psi=model(x1).squeeze(1)\n",
    "    los=loss_func(x1,psi)\n",
    "    upd=update(x1,psi,delta)\n",
    "    #print('unit print=',psi.shape,los.shape,upd.shape)\n",
    "    return psi,los,upd\n",
    "\n",
    "def loss_func_total(u,model,delta):\n",
    "  loss=torch.zeros(M,ite+1)\n",
    "  psi=torch.zeros(M,ite+1)\n",
    "  for i in range(ite+1):\n",
    "    if(i!=ite):\n",
    "        psi_run,loss_run,u_run=unit(u,model,delta)\n",
    "        #print('los func=',psi_run.shape,loss_run.shape)\n",
    "        loss[:,i]=loss_run\n",
    "        #print(loss)\n",
    "        psi[:,i]=psi_run\n",
    "        #print(psi)\n",
    "        u=u_run\n",
    "    #print(u)\n",
    "    else:\n",
    "    #print(torch.sum(psi,dim=1),R.squeeze(1))\n",
    "        psi_ter=R.squeeze(1)-torch.sum(psi,dim=1)\n",
    "        loss_ter=loss_func(u,psi_ter)\n",
    "      #print('ter',loss_ter.shape)\n",
    "  #print(torch.sum(loss,dim=1))\n",
    "  #loss1=torch.sum(loss,dim=1)\n",
    "  loss1=torch.sum(loss,dim=1)+loss_ter\n",
    "  #print(loss.shape)\n",
    "  return torch.mean(loss1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1846f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_coarse(X0,M1):    \n",
    "    t_test=torch.zeros([M1,1])\n",
    "    D_test=torch.zeros([M1,1])\n",
    "    #D_test=torch.FloatTensor(M,1).uniform_(0.1,0.5)   #Price impact D_t\n",
    "    #R_test=torch.FloatTensor(M1,1).uniform_(50,100)   #remaining balance R_t   #To get a positive solution R_t has to be greater than D_t\n",
    "    R_test=torch.linspace(X0*0.90, X0*1.1, steps=M1).unsqueeze(1)\n",
    "    x_test=torch.cat((t_test,D_test,R_test),dim=1)\n",
    "    #print('Input=',x_test,'\\n')\n",
    "    a=torch.zeros(M1,ite+1)\n",
    "    #print(a.shape)\n",
    "    for i in range(ite+1):\n",
    "      if(i!=ite):\n",
    "        #print(x_test)\n",
    "        soln_pred=model_coarse.eval()(x_test).squeeze(1).detach()\n",
    "        a[:,i]=soln_pred\n",
    "        x_test=update(x_test,soln_pred,delta_c)\n",
    "        #print(x_test,'\\n')\n",
    "      else:\n",
    "        a[:,i]=R_test.squeeze(1)-torch.sum(a,dim=1)\n",
    "\n",
    "    x1=R_test.squeeze().numpy()\n",
    "    t_soln=np.zeros((M1,ite+1))\n",
    "    return x_test,a\n",
    "\n",
    "#     for i in range(M1):\n",
    "#       print('size of total order=',x1[i])\n",
    "\n",
    "    #   num_soln,num_cost=numeric_soln(x1[i],ite+1)\n",
    "    #   n_soln[i,:]=num_soln\n",
    "    #   print('Numeric_soln',num_soln,'sum of all execution=',np.sum(num_soln))\n",
    "    #   print('Numeric_cost=',num_cost,'\\n')\n",
    "    #   c_num[i]=num_cost\n",
    "\n",
    "    #   t_soln[i,:]=closed_form(x1[i])\n",
    "    #   print('closed form soln=',t_soln[i,:],'sum of all execution=',np.sum(t_soln[i,:]))\n",
    "    #   true_cost=closed_cost(t_soln[i,:])\n",
    "    #   c_true[i]=true_cost\n",
    "    #   print('Closed form cost=',true_cost,'\\n')\n",
    "\n",
    "      #print('predicted soln=',a[i,:],'sum of all execution=',torch.sum(a[i,:]).detach())\n",
    "    #   pred_cost=predicted_cost(a[i,:])\n",
    "    #   print('predicted cost=',pred_cost.detach().numpy(),'\\n')\n",
    "    #   c_pred[i]=pred_cost\n",
    "    # print(x_test,a)\n",
    "    # print(torch.min(a),torch.min(x_test[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_c,c_soln=testing_coarse(X0,5)\n",
    "# print(c_soln.shape)\n",
    "# print(x_c,c_soln,torch.sum(a,1))\n",
    "D_max=torch.max(x_c[:,1])\n",
    "R_min=torch.min(c_soln)\n",
    "R_max=torch.max(c_soln)\n",
    "t=torch.zeros([M,1])\n",
    "D=torch.FloatTensor(M,1).uniform_(0.0,D_max)\n",
    "#D=torch.FloatTensor(M,1).uniform_(0.1,0.5)   #Price impact D_t\n",
    "R=torch.FloatTensor(M,1).uniform_(R_min,R_max)   #remaining balance R_t   #To get a positive solution R_t has to be greater than D_t\n",
    "x=torch.cat((t,D,R),dim=1)\n",
    "print('coarse soution=',c_soln[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f477c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "lr1 = 8e-3\n",
    "max_epoch = 8000\n",
    "optimizer = optim.Adam(model_f.parameters(), lr1)\n",
    "\n",
    "#print(psi.shape)\n",
    "for epoch in range(max_epoch):\n",
    "  optimizer.zero_grad()\n",
    "  cost=loss_func_total(x,model_f,delta_f)\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "  #print(loss.item())\n",
    "  if (epoch % 100==0):\n",
    "    print(\"At epoch {} the mean error is {:,}.\".format(epoch,cost.detach()))\n",
    "end=time.time()\n",
    "print('time elapsed=',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285e4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_f, 'model_N10_fine.pth')\n",
    "model_fin = torch.load('model_N10_fine.pth')\n",
    "#model_fin=model_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df69d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closed_form(X,ite):\n",
    "  alpa=np.exp(-rho*T/ite)\n",
    "  p=np.zeros(ite+1)\n",
    "  p[0]=X/((ite-1)*(1-alpa)+2)\n",
    "  p[ite]=X/((ite-1)*(1-alpa)+2)\n",
    "  for j in range(1,ite):\n",
    "    p[j]=p[0]*(1-alpa)\n",
    "  return p\n",
    "\n",
    "def cost(y,ite):\n",
    "  cost1=0.0\n",
    "  D1=0.0\n",
    "  for i in range(ite+1):\n",
    "    cost1+=D1*y[i]+(kappa/2.0)*np.power(y[i],2)\n",
    "    D1=(D1+kappa*y[i])*np.exp(-rho*T/ite)\n",
    "  return cost1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e4b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_fine_forward(c_soln,ite):    \n",
    "    f_soln=[]\n",
    "    c=c_soln[2]\n",
    "    x_test=torch.tensor([[0.0,0.0,c[0]]])\n",
    "    for i in range(ite+1):\n",
    "        if (i!=0):\n",
    "            x_test[0,2]=x_test[0,2]+c[i] #Adding previous remaining balance to the current balance\n",
    "        if (i!=ite):         \n",
    "            for j in range(ite):\n",
    "                #print(i,x_test)\n",
    "                soln_pred=model_fin.eval()(x_test).squeeze(1).detach()\n",
    "                f_soln.append(soln_pred)\n",
    "                x_test=update(x_test,soln_pred,delta_f)\n",
    "            #print(x_test[0,2])\n",
    "    f_soln.append(x_test[0,2].view(1))\n",
    "    f_soln= np.array([tensor.numpy() for tensor in f_soln]).reshape(len(f_soln))\n",
    "    return f_soln\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006892a2",
   "metadata": {},
   "source": [
    "# Results from forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c202172",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_soln=c_soln[2].numpy()\n",
    "print('coarse soln',coarse_soln, 'Total=',np.sum(coarse_soln))\n",
    "fine_soln=testing_fine_forward(c_soln,ite)\n",
    "true_soln=closed_form(10000,100)\n",
    "print('\\nFine solution=', fine_soln, 'Total=',np.sum(fine_soln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab76b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_step1=np.linspace(0,1,num=11)\n",
    "t_step2=np.linspace(0,1,num=101)\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(t_step1,coarse_soln,color='red',width=0.01,label='coarse')\n",
    "plt.bar(t_step2,fine_soln,color='green',width=0.005,label='fine')\n",
    "plt.bar(t_step2,true_soln,color='blue',width=0.002,label='true')\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('Optimal orders')\n",
    "plt.title('Forward Pass')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdacda2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_cost=cost(true_soln,100)\n",
    "# fine_cost=cost(fine_soln,100)\n",
    "# print('True Cost= %0.2f'% true_cost, 'Fine grid cost= %0.2f'% fine_cost)\n",
    "# error=np.abs(true_cost-fine_cost)\n",
    "# per_error=100*error/true_cost\n",
    "# print('Cost Error= %0.2f'% error)\n",
    "# print('Percentage error= %0.2f'% per_error,'%' )\n",
    "# D_N=data[0,2].detach().numpy()\n",
    "# derivative=kappa*(fine_soln[0])-((10000-sum(fine_soln[i] for i in range(len(fine_soln)-1)))+D_N)\n",
    "# print('\\nDerivative of loss with respect to x0 =',derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f44f2",
   "metadata": {},
   "source": [
    "# Strategy 2 (Adding two consecutive terms to predict the block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_fine_first_last(c_soln,ite):    \n",
    "    f_soln=[]\n",
    "    c=c_soln[2]\n",
    "    x_test=torch.tensor([[0.0,0.0,c[0]]])\n",
    "    #x_test[0,2]=x_test[0,2]+c[ite]\n",
    "    for i in range(ite):       \n",
    "        x_test[0,2]=x_test[0,2]+c[i+1] #Adding previous remaining balance to the current balance         \n",
    "        for j in range(ite):\n",
    "            #print(i,x_test)\n",
    "            soln_pred=model_fin.eval()(x_test).squeeze(1).detach()\n",
    "            f_soln.append(soln_pred)\n",
    "            x_test=update(x_test,soln_pred,delta_f)\n",
    "            #print(x_test[0,2])\n",
    "    f_soln.append(x_test[0,2].view(1))\n",
    "    f_soln= np.array([tensor.numpy() for tensor in f_soln]).reshape(len(f_soln))\n",
    "    return f_soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine_soln=testing_fine_first_last(c_soln,ite)\n",
    "# true_soln=closed_form(10000,100)\n",
    "# print('\\nFine solution=', fine_soln, fine_soln.shape,'Total=',np.sum(fine_soln))\n",
    "# print('\\nDifference between true and fine=',(fine_soln-true_soln))\n",
    "# D_N=data[0,2].detach().numpy()\n",
    "# derivative=kappa*(fine_soln[0])-((10000-sum(fine_soln[i] for i in range(len(fine_soln)-1)))+D_N)\n",
    "# print('\\nDerivative of loss with respect to x0 =',derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc7688",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_step1=np.linspace(0,1,num=11)\n",
    "t_step2=np.linspace(0,1,num=101)\n",
    "plt.figure(figsize=(14,6))\n",
    "#plt.bar(t_step1,coarse_soln,color='red',width=0.01,label='coarse')\n",
    "plt.bar(t_step2,fine_soln,color='green',width=0.005,label='fine')\n",
    "plt.bar(t_step2,true_soln,color='yellow',width=0.002,label='true')\n",
    "plt.xlabel('Time steps')\n",
    "plt.ylabel('Optimal orders')\n",
    "plt.title('Forward Pass')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e97e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_cost=cost(true_soln,100)\n",
    "# fine_cost=cost(fine_soln,100)\n",
    "# print('True Cost= %0.2f'% true_cost, 'Fine grid cost= %0.2f'% fine_cost)\n",
    "# error=np.abs(true_cost-fine_cost)\n",
    "# per_error=100*error/true_cost\n",
    "# print('Cost Error= %0.2f'% error)\n",
    "# print('Percentage error= %0.2f'% per_error,'%' )\n",
    "# D_N=data[0,2].detach().numpy()\n",
    "# derivative=kappa*(fine_soln[0])-((10000-sum(fine_soln[i] for i in range(len(fine_soln)-1)))+D_N)\n",
    "# print('\\nDerivative of loss with respect to x0 =',derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a240b8",
   "metadata": {},
   "source": [
    "# Strategy 3: Updated (Adding First and last balance and then doing the forward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce39fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_fine_first_last(c_soln,ite):    \n",
    "    f_soln=[]\n",
    "    c=c_soln[2]\n",
    "    x_test=torch.tensor([[0.0,0.0,c[0]]])\n",
    "    x_test[0,2]=x_test[0,2]+c[ite]\n",
    "    for i in range(ite):       \n",
    "        if (i!=0):\n",
    "            x_test[0,2]=x_test[0,2]+c[i] #Adding previous remaining balance to the current balance         \n",
    "        for j in range(ite):\n",
    "            #print(i,x_test)\n",
    "            soln_pred=model_fin.eval()(x_test).squeeze(1).detach()\n",
    "            f_soln.append(soln_pred)\n",
    "            x_test=update(x_test,soln_pred,delta_f)\n",
    "            #print(x_test[0,2])\n",
    "    f_soln.append(x_test[0,2].view(1))\n",
    "    f_soln= np.array([tensor.numpy() for tensor in f_soln]).reshape(len(f_soln))\n",
    "    return x_test,f_soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761849da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data,fine_soln=testing_fine_first_last(c_soln,ite)\n",
    "D_N=data[0,2].detach().numpy()\n",
    "derivative_fin=kappa*(fine_soln[0])-((10000-sum(fine_soln[i] for i in range(len(fine_soln)-1)))+D_N)\n",
    "derivative_true=kappa*(true_soln[0])-((10000-sum(true_soln[i] for i in range(len(true_soln)-1)))+D_N)\n",
    "print('\\nDerivative of loss func with respect to x0 (True soln) =',derivative_true)\n",
    "print('\\nDerivative of loss func with respect to x0 (Fine soln) =',derivative_fin)\n",
    "true_soln=closed_form(10000,100)\n",
    "print('\\nFine solution=', fine_soln,'Total=',np.sum(fine_soln))\n",
    "print('\\nTrue solution=', true_soln,'Total=',np.sum(true_soln))\n",
    "print('\\nDifference between true and fine=',(fine_soln-true_soln))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b6c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_step1=np.linspace(0,1,num=11)\n",
    "t_step2=np.linspace(0,100,num=101)\n",
    "name='N10_T1_K1_rho5_neo20.eps'\n",
    "f, ax = plt.subplots(figsize=(7,3),dpi=600)\n",
    "#plt.bar(t_step1,coarse_soln,color='red',width=0.01,label='coarse')\n",
    "ax.bar(t_step2,fine_soln,color='green',width=0.8,label='fine-grid solution',alpha=0.5)\n",
    "ax.bar(t_step2,true_soln,color='red',width=0.4,label='closed-form solution',alpha=0.5)\n",
    "ax.set_xlabel('Time steps')\n",
    "ax.set_ylabel('Optimal orders (log-scale)')\n",
    "f.suptitle('Multiscale Solution for the order of size X={:,}'.format(X0))\n",
    "ax.legend()\n",
    "# plt.savefig(name,format='eps')  \n",
    "ax.set_yscale('log')\n",
    "# plt.show()\n",
    "tikzplotlib.save(\"LOB_Linear.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee84357d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_cost=cost(true_soln,100)\n",
    "fine_cost=cost(fine_soln,100)\n",
    "print('True Cost= %0.2f'% true_cost, 'Fine grid cost= %0.2f'% fine_cost)\n",
    "error=np.abs(true_cost-fine_cost)\n",
    "per_error=100*error/true_cost\n",
    "print('Cost Error = %0.2f'% error)\n",
    "print('Percentage Error = %0.2f'% per_error,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfae8371",
   "metadata": {},
   "source": [
    "# The derivative of the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e334358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The derivative of the loss function with respect to x0 = kappa*x0-[kappa(X-(x0+x1+......+x_{N-1})+DN)]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cdad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
